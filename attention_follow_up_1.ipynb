{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attention_follow_up_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOjysHXhMORyMTc2gh5h16i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chuck2Win/attention/blob/attention_follow_up_1/attention_follow_up_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldWZU2pOGp4l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "outputId": "557492e0-f382-4109-9d5f-664c014d5e7d"
      },
      "source": [
        "import numpy as np\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchtext\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "import os\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "from matplotlib import pyplot as plt\n",
        "import json\n",
        "# google drive와 연동\n",
        "drive.mount('/content/gdrive')\n",
        "print(os.getcwd())\n",
        "# os.chdir('..') ..은 상위 폴더로 이동\n",
        "os.chdir('./gdrive/My Drive/attention_from_master_')\n",
        "print(os.getcwd())\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/My Drive/attention_from_master_\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-f70561cf6116>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# os.chdir('..') ..은 상위 폴더로 이동\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./gdrive/My Drive/attention_from_master_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './gdrive/My Drive/attention_from_master_'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0HB072iG_zr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "4dfce77b-6de5-4f01-d88c-da11c488a265"
      },
      "source": [
        "! pip install sentencepiece\n",
        "import sentencepiece as spm\n",
        "# testing\n",
        "vocab_file = \"./kowiki.model\"\n",
        "vocab = spm.SentencePieceProcessor()\n",
        "vocab.load(vocab_file)\n",
        "\n",
        "lines = [\n",
        "  \"겨울이 되어서 날씨가 무척 추워요.\",\n",
        "  \"이번 성탄절은 화이트 크리스마스가 될까요?\",\n",
        "  \"겨울에 감기 조심하시고 행복한 연말 되세요.\"\n",
        "]\n",
        "inputs=[]\n",
        "for line in lines:\n",
        "  pieces = vocab.encode_as_pieces(line)\n",
        "  ids = vocab.encode_as_ids(line)\n",
        "  inputs.append(torch.tensor(ids))\n",
        "  print(line)\n",
        "  print(pieces)\n",
        "  print(ids)\n",
        "  print() # 단어를 subword 단위로 잘 쪼개는것을 확인할 수 있습니다."
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.86)\n",
            "겨울이 되어서 날씨가 무척 추워요.\n",
            "['▁겨울', '이', '▁되어', '서', '▁날', '씨', '가', '▁무', '척', '▁추', '워', '요', '.']\n",
            "[3099, 3582, 604, 3596, 684, 4009, 3593, 109, 4189, 206, 3951, 3754, 3584]\n",
            "\n",
            "이번 성탄절은 화이트 크리스마스가 될까요?\n",
            "['▁이번', '▁성', '탄', '절', '은', '▁화', '이트', '▁크리스', '마', '스가', '▁될', '까', '요', '?']\n",
            "[3228, 86, 3961, 3917, 3598, 264, 669, 1940, 3658, 777, 1442, 3788, 3754, 4243]\n",
            "\n",
            "겨울에 감기 조심하시고 행복한 연말 되세요.\n",
            "['▁겨울', '에', '▁감', '기', '▁조', '심', '하', '시', '고', '▁행', '복', '한', '▁연', '말', '▁되', '세', '요', '.']\n",
            "[3099, 3585, 208, 3599, 53, 3826, 3590, 3608, 3594, 235, 3865, 3597, 61, 3817, 448, 3676, 3754, 3584]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4lk7-6fHEDR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8f964221-4323-4374-f050-0ee4ccac051c"
      },
      "source": [
        "print(len(vocab)) # 8007\n",
        "print(len(ids))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8007\n",
            "18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_MIYQwdNUXJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "ca069d45-d60c-4056-b09f-80946476983b"
      },
      "source": [
        "# 입력 길이가 다르므로 입력 최대 길이에 맟춰 padding(0)을 추가 해 줌\n",
        "inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n",
        "print(inputs)\n",
        "print(inputs.shape) # 3, 18 -> 가장 긴 문장을 기준으로 padding 해준다."
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[3099, 3582,  604, 3596,  684, 4009, 3593,  109, 4189,  206, 3951, 3754,\n",
            "         3584,    0,    0,    0,    0,    0],\n",
            "        [3228,   86, 3961, 3917, 3598,  264,  669, 1940, 3658,  777, 1442, 3788,\n",
            "         3754, 4243,    0,    0,    0,    0],\n",
            "        [3099, 3585,  208, 3599,   53, 3826, 3590, 3608, 3594,  235, 3865, 3597,\n",
            "           61, 3817,  448, 3676, 3754, 3584]])\n",
            "torch.Size([3, 18])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deyDJ6gRI5_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# positional encoding을 해줘야한다.\n",
        "def positional_encoding(seq_len,embedding_dim):\n",
        "    # input shape : [n,seq_len]\n",
        "    #output=torch.zeros((seq_len+1,embedding_dim),device=device)\n",
        "    output=torch.zeros((seq_len,embedding_dim),device=device)\n",
        "    def for_insert(output):\n",
        "        m,n=output.shape\n",
        "        for i in range(m):\n",
        "            for j in range(n):\n",
        "                output[i,j]=i/10000**(2*j/embedding_dim)\n",
        "        return output\n",
        "    pe=for_insert(output)\n",
        "    pe[:,0::2]=torch.sin(pe[:,0::2])\n",
        "    pe[:,1::2]=torch.cos(pe[:,1::2])\n",
        "    # pe[0,:]=0. 없애고 진행해보자\n",
        "    return pe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPNQkNjiL_in",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "71a81ce8-8679-4640-8ffc-0f99cbc510a4"
      },
      "source": [
        "device='cpu'\n",
        "positional_encoding(64,300)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
              "          0.0000e+00,  1.0000e+00],\n",
              "        [ 8.4147e-01,  5.8943e-01,  7.7356e-01,  ...,  1.0000e+00,\n",
              "          1.1307e-08,  1.0000e+00],\n",
              "        [ 9.0930e-01, -3.0515e-01,  9.8045e-01,  ...,  1.0000e+00,\n",
              "          2.2613e-08,  1.0000e+00],\n",
              "        ...,\n",
              "        [-9.6612e-01,  6.8334e-01, -5.1718e-01,  ...,  1.0000e+00,\n",
              "          6.8970e-07,  1.0000e+00],\n",
              "        [-7.3918e-01, -1.8701e-01, -9.8982e-01,  ...,  1.0000e+00,\n",
              "          7.0101e-07,  1.0000e+00],\n",
              "        [ 1.6736e-01, -9.0380e-01, -7.3737e-01,  ...,  1.0000e+00,\n",
              "          7.1232e-07,  1.0000e+00]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
              "          0.0000e+00,  1.0000e+00],\n",
              "        [ 8.4147e-01,  5.8943e-01,  7.7356e-01,  ...,  1.0000e+00,\n",
              "          1.1307e-08,  1.0000e+00],\n",
              "        [ 9.0930e-01, -3.0515e-01,  9.8045e-01,  ...,  1.0000e+00,\n",
              "          2.2613e-08,  1.0000e+00],\n",
              "        ...,\n",
              "        [-9.6612e-01,  6.8334e-01, -5.1718e-01,  ...,  1.0000e+00,\n",
              "          6.8970e-07,  1.0000e+00],\n",
              "        [-7.3918e-01, -1.8701e-01, -9.8982e-01,  ...,  1.0000e+00,\n",
              "          7.0101e-07,  1.0000e+00],\n",
              "        [ 1.6736e-01, -9.0380e-01, -7.3737e-01,  ...,  1.0000e+00,\n",
              "          7.1232e-07,  1.0000e+00]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCtihPHIMg7Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "d91a3c5b-74ab-4038-a173-d4d5d3590484"
      },
      "source": [
        "# 이를 토대로 positional embedding을 구한다.\n",
        "'''\n",
        "위에서 구해진 position encoding 값을 이용해 position embedding을 생성합니다. \n",
        "학습되는 값이 아니므로 freeze옵션을 True로 설정 합니다.\n",
        "입력 inputs과 동일한 크기를 갖는 positions값을 구합니다.\n",
        "input값 중 pad(0)값을 찾습니다.\n",
        "positions값중 pad부분은 0으로 변경 합니다. (mask를 활용)\n",
        "positions값에 해당하는 embedding값을 구합니다\n",
        "'''"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n위에서 구해진 position encoding 값을 이용해 position embedding을 생성합니다. \\n학습되는 값이 아니므로 freeze옵션을 True로 설정 합니다.\\n입력 inputs과 동일한 크기를 갖는 positions값을 구합니다.\\ninput값 중 pad(0)값을 찾습니다.\\npositions값중 pad부분은 0으로 변경 합니다. (mask를 활용)\\npositions값에 해당하는 embedding값을 구합니다\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n위에서 구해진 position encoding 값을 이용해 position embedding을 생성합니다. \\n학습되는 값이 아니므로 freeze옵션을 True로 설정 합니다.\\n입력 inputs과 동일한 크기를 갖는 positions값을 구합니다.\\ninput값 중 pad(0)값을 찾습니다.\\npositions값중 pad부분은 0으로 변경 합니다. (mask를 활용)\\npositions값에 해당하는 embedding값을 구합니다\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQmJPNFQMyin",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708
        },
        "outputId": "fe6cecb4-e25e-4385-eecf-c2f1272de500"
      },
      "source": [
        "############################# input embedding ##########################################\n",
        "n_vocab=len(vocab)\n",
        "embedding=128\n",
        "seq_len=64 # 아싸리 크게 하면 되겠네. 딱 타이트하게 하지 말고...\n",
        "Embedding=nn.Embedding(n_vocab,embedding,padding_idx=0)\n",
        "Embedding.weight.required_grad=False\n",
        "inputs_embedding=Embedding(inputs) \n",
        "############################# positional embedding ##########################################\n",
        "pos_encoding=positional_encoding(seq_len,embedding)\n",
        "positional_embedding=nn.Embedding.from_pretrained(pos_encoding,freeze=True)\n",
        "positions=torch.arange(inputs.size(1)).expand(inputs.size(0),inputs.size(1))+1 # inputs size : n,seq_len\n",
        "print(positions)\n",
        "pos_mask=inputs.eq(0) # mask shape : n,seq_len : 3, 18\n",
        "positions.masked_fill_(pos_mask,0)  # (어차피 0에는 무엇을 곱해도 0이 되니깐)->아니지. 그리고 1을 더했다.. 그전에\n",
        "print(positions)\n",
        "pos_embedding=positional_embedding(positions)\n",
        "print(pos_embedding[1]) # 보게 되면 0에 해당하는 부분은 0,1,0,1로 반복되고 있다."
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18],\n",
            "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18],\n",
            "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18]])\n",
            "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,  0,  0,  0,  0,  0],\n",
            "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,  0,  0,  0,  0],\n",
            "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18]])\n",
            "tensor([[ 8.4147e-01,  6.4791e-01,  6.8156e-01,  ...,  1.0000e+00,\n",
            "          1.3335e-08,  1.0000e+00],\n",
            "        [ 9.0930e-01, -1.6044e-01,  9.9748e-01,  ...,  1.0000e+00,\n",
            "          2.6670e-08,  1.0000e+00],\n",
            "        [ 1.4112e-01, -8.5580e-01,  7.7827e-01,  ...,  1.0000e+00,\n",
            "          4.0006e-08,  1.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
            "          0.0000e+00,  1.0000e+00],\n",
            "        [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
            "          0.0000e+00,  1.0000e+00],\n",
            "        [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
            "          0.0000e+00,  1.0000e+00]])\n",
            "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18],\n",
            "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18],\n",
            "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18]])\n",
            "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,  0,  0,  0,  0,  0],\n",
            "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,  0,  0,  0,  0],\n",
            "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18]])\n",
            "tensor([[ 8.4147e-01,  6.4791e-01,  6.8156e-01,  ...,  1.0000e+00,\n",
            "          1.3335e-08,  1.0000e+00],\n",
            "        [ 9.0930e-01, -1.6044e-01,  9.9748e-01,  ...,  1.0000e+00,\n",
            "          2.6670e-08,  1.0000e+00],\n",
            "        [ 1.4112e-01, -8.5580e-01,  7.7827e-01,  ...,  1.0000e+00,\n",
            "          4.0006e-08,  1.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
            "          0.0000e+00,  1.0000e+00],\n",
            "        [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
            "          0.0000e+00,  1.0000e+00],\n",
            "        [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
            "          0.0000e+00,  1.0000e+00]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "422ILQbeSWwl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "outputId": "4091f794-86c6-4970-a75f-e155df3a3081"
      },
      "source": [
        "# positional embedding과 token embedding을 더해야지\n",
        "input_sums=inputs_embedding+pos_embedding \n",
        "# anyway, padding이 된 부분에 해당되는 부분이 0 값이 된 것은 아니다.. -> 그렇기 때문에...\n",
        "inputs[0]\n",
        "input_sums[0]"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.7790,  1.4683, -0.1738,  ...,  1.8388, -1.5923,  1.0434],\n",
              "        [ 2.8732,  0.5689,  0.5324,  ...,  1.9610, -1.2025,  1.6823],\n",
              "        [-0.4929, -1.9724,  0.8430,  ..., -0.4396,  0.8397,  1.8953],\n",
              "        ...,\n",
              "        [ 0.0000,  1.0000,  0.0000,  ...,  1.0000,  0.0000,  1.0000],\n",
              "        [ 0.0000,  1.0000,  0.0000,  ...,  1.0000,  0.0000,  1.0000],\n",
              "        [ 0.0000,  1.0000,  0.0000,  ...,  1.0000,  0.0000,  1.0000]],\n",
              "       grad_fn=<SelectBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.9317, -0.9484,  1.4497,  ...,  0.3678,  0.4071,  0.3801],\n",
              "        [ 3.0645,  0.3304,  1.3181,  ...,  2.2471, -1.7370,  1.3173],\n",
              "        [ 1.3605, -1.7109, -0.1601,  ..., -0.0892,  0.1210,  0.7888],\n",
              "        ...,\n",
              "        [ 0.0000,  1.0000,  0.0000,  ...,  1.0000,  0.0000,  1.0000],\n",
              "        [ 0.0000,  1.0000,  0.0000,  ...,  1.0000,  0.0000,  1.0000],\n",
              "        [ 0.0000,  1.0000,  0.0000,  ...,  1.0000,  0.0000,  1.0000]],\n",
              "       grad_fn=<SelectBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LFPW0tIU0Fl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e88fd329-a7a9-46c0-c6b9-1c1ff94c5c0e"
      },
      "source": [
        "Q=input_sums # shape : 3, 18, 128\n",
        "K=input_sums # inputs shape : 3, 18\n",
        "V=input_sums\n",
        "# inputs.shape : batch_size, seq_len\n",
        "attention_mask=inputs.eq(0).unsqueeze(1).expand(Q.size(0),Q.size(1),K.size(1)) # shape : 3, 18, 18\n",
        "# print(get_attention_pad_mask(Q,K))\n",
        "print(attention_mask[0].shape)\n",
        "print(attention_mask[0])"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([18, 18])\n",
            "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False,  True,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False,  True,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False,  True,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False,  True,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False,  True,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False,  True,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False,  True,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False,  True,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False,  True,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False,  True,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False,  True,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False,  True,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False,  True,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False,  True,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False,  True,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False,  True,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False,  True,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False,  True,  True,  True,  True,  True]])\n",
            "torch.Size([18, 18])\n",
            "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False,  True,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False,  True,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False,  True,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False,  True,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False,  True,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False,  True,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False,  True,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False,  True,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False,  True,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False,  True,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False,  True,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False,  True,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False,  True,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False,  True,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False,  True,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False,  True,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False,  True,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False,  True,  True,  True,  True,  True]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ev-98oGKWfTU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class scaled_dot_product_attention(nn.Module): # 고민 요소\n",
        "    # 나의 초안에서 attention 함수를 차용해 옴\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "               \n",
        "    def forward(self,Q,K,V,attention_mask):\n",
        "        # Q,K - shape [n,seq_len,dk] // V - shape [n,seq_len,dv]\n",
        "        # self attention 이므로 dk=dv(=embedding)\n",
        "        scores=Q.matmul(K.transpose(-2,-1))/torch.sqrt(torch.tensor(Q.shape[-1]).float())\n",
        "        # 여기에서 attention mask는 padding이 된 부분을 0으로 처리한 것\n",
        "        # attention mask shape : n,seq_len,seq_len\n",
        "        # 00011\n",
        "        # 00011\n",
        "        # 00011\n",
        "        # 00011\n",
        "        # 00011 \n",
        "        # 요론 모양\n",
        "        scores.masked_fill_(attention_mask,-1e-9)\n",
        "        weight=F.softmax(scores,dim=-1)\n",
        "        # weight : n, seq_len, seq_len\n",
        "        attention_score=weight.matmul(V)\n",
        "        # attention score : n,seq_len,dv\n",
        "        return attention_score, weight"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aW--ols2aRTC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 나는 multi head attention을 안했었는데, 병렬로 처리하기 위해선 더 좋으니깐\n",
        "# 쩐다...★\n",
        "class multi_head_attention(nn.Module): # 고민 요소\n",
        "    # 나의 초안에서 attention 함수를 차용해 옴\n",
        "    def __init__(self,batch_size,n_head,d_head,d_k):\n",
        "        super().__init__()\n",
        "        self.batch_size=batch_size\n",
        "        self.n_head=n_head\n",
        "        self.d_head=d_head\n",
        "        self.d_k=d_k\n",
        "        self.W_Q=nn.Linear(d_k,n_head*d_head)\n",
        "        self.W_K=nn.Linear(d_k,n_head*d_head)\n",
        "        self.W_V=nn.Linear(d_k,n_head*d_head)\n",
        "        self.W_0=nn.Linear(n_head*d_head,n_head*d_head) # n_head*d_head = embedding\n",
        "        self.sdpa=scaled_dot_product_attention()\n",
        "    def forward(self,Q,K,V,attention_mask):\n",
        "        # Q,K - shape [n,seq_len,dk] // V - shape [n,seq_len,dv]\n",
        "        # self attention 이므로 dk=dv(=embedding)\n",
        "        # attention_mask shape : [n,seq_len,seq_len]\n",
        "        qs=self.W_Q(Q).reshape(self.batch_size,-1,self.n_head,self.d_head).transpose(1,2) # ★\n",
        "        ks=self.W_K(K).reshape(self.batch_size,-1,self.n_head,self.d_head).transpose(1,2)\n",
        "        vs=self.W_V(V).reshape(self.batch_size,-1,self.n_head,self.d_head).transpose(1,2)\n",
        "        attention_mask=attention_mask.unsqueeze(1).repeat(1,self.n_head,1,1) # attention_mask shape : [n,self.n_head,seq_len,seq_len] <- mask도 동일하게 multi head로 확장시켜 나가야지\n",
        "        context,weight=self.sdpa(qs,ks,vs,attention_mask) # context shape : n, n_head, seq_len, d_head\n",
        "        context=context.transpose(1,2).reshape(self.batch_size,-1,self.n_head*self.d_head)\n",
        "        output=self.W_0(context)\n",
        "        return output, weight"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqZA3ucrbm8C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bcd7ea6e-c520-4d62-c54a-df4ae8e37907"
      },
      "source": [
        "mm=multi_head_attention(3,2,64,128)\n",
        "mm.forward(Q,K,V,attention_mask)[0].shape"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 18, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 18, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBJmZemFfWDt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# masked multi_head attention\n",
        "def get_attention_decoder_mask(seq):\n",
        "    # seq : batch_size, query_seq_len\n",
        "    subsequent_mask=torch.ones_like(seq).unsqueeze(-1).expand(seq.size(0),seq.size(1),seq.size(1))\n",
        "    # mask : batch_size, seq_len, seq_len\n",
        "    mask=subsequent_mask.triu(diagonal=1)\n",
        "    return mask   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yutc5dVfpjx0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "8cd2590c-ccc6-4c08-ac31-6a857007b5a6"
      },
      "source": [
        "attn_dec_mask=get_attention_decoder_mask(inputs)\n",
        "attn_dec_mask.shape\n",
        "attn_dec_mask[0]"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k59OKTT3qbbM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        },
        "outputId": "e1f05444-2144-4f5f-c7f0-711e0d4c4b94"
      },
      "source": [
        "attn_pad_mask=inputs.eq(0).unsqueeze(1).expand(Q.size(0),Q.size(1),K.size(1))\n",
        "print(attn_pad_mask[1])"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False,  True,  True,  True,  True]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFaFuUOf3-D8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pad mask와 decoder mask를 합쳐야 한다.\n",
        "attention_decoder_mask=get_attention_decoder_mask(inputs)\n",
        "attn_mask=torch.gt((attn_pad_mask+attention_decoder_mask),0) # element wise로 0보다 크면 True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "td2Wp6n-wzXR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4fa4a0ca-8c67-4257-fa22-d70ae11e0b07"
      },
      "source": [
        "\"\"\" feed forward \"\"\"\n",
        "class PoswiseFeedForwardNet(nn.Module):\n",
        "    def __init__(self, d_hidn):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv1d(in_channels=self.config.d_hidn, out_channels=self.config.d_hidn * 4, kernel_size=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=self.config.d_hidn * 4, out_channels=self.config.d_hidn, kernel_size=1)\n",
        "        self.active = F.gelu\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # (bs, d_ff, n_seq)\n",
        "        output = self.active(self.conv1(inputs.transpose(1, 2)))\n",
        "        # (bs, n_seq, d_hidn)\n",
        "        output = self.conv2(output).transpose(1, 2)\n",
        "        # (bs, n_seq, d_hidn)\n",
        "        return output"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]],\n",
              "\n",
              "        [[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]],\n",
              "\n",
              "        [[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6d6auS76HDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ? torch.gt()\n",
        "attn_mask=torch.gt((attention_pad_mask+attention_decoder_mask),0) # torch.gt(input,other)=> input>other인 경우 element wise\n",
        "print(attn_mask[0].long())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYkdHZZI8NyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Config(dict):\n",
        "    __getitem__=dict.__getitem__\n",
        "    __setitem__=dict.__setitem__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irfLmQaE8cOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config=Config({\"n_enc_vocab\": len(vocab),\n",
        "    \"n_dec_vocab\": len(vocab),\n",
        "    \"n_enc_seq\": 256,\n",
        "    \"n_dec_seq\": 256,\n",
        "    \"n_layer\": 6,\n",
        "    \"d_hidden\": 256,\n",
        "    \"padding_idx\": 0,\n",
        "    \"d_feedforward\": 1024,\n",
        "    \"n_head\": 4,\n",
        "    \"d_head\": 64,\n",
        "    \"dropout\": 0.1,\n",
        "    \"layer_norm_epsilon\": 1e-12\n",
        "})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTmvJVMf83RC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}