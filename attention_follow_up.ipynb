{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attention_follow_up.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMjsPaO2TTgrCdHUGQK4Nzb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chuck2Win/attention/blob/attention_follow_up/attention_follow_up.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldWZU2pOGp4l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "d137985b-3de7-499d-ea08-25f1305921bd"
      },
      "source": [
        "import numpy as np\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchtext\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "import os\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "from matplotlib import pyplot as plt\n",
        "import json\n",
        "# google drive와 연동\n",
        "drive.mount('/content/gdrive')\n",
        "print(os.getcwd())\n",
        "# os.chdir('..') ..은 상위 폴더로 이동\n",
        "os.chdir('./gdrive/My Drive/attention_from_master_')\n",
        "print(os.getcwd())\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "/content\n",
            "/content/gdrive/My Drive/attention_from_master_\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0HB072iG_zr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "9352cd67-b963-40a4-a955-947ce3c27428"
      },
      "source": [
        "! pip install sentencepiece\n",
        "import sentencepiece as spm\n",
        "# testing\n",
        "vocab_file = \"./kowiki.model\"\n",
        "vocab = spm.SentencePieceProcessor()\n",
        "vocab.load(vocab_file)\n",
        "\n",
        "lines = [\n",
        "  \"겨울이 되어서 날씨가 무척 추워요.\",\n",
        "  \"이번 성탄절은 화이트 크리스마스가 될까요?\",\n",
        "  \"겨울에 감기 조심하시고 행복한 연말 되세요.\"\n",
        "]\n",
        "inputs=[]\n",
        "for line in lines:\n",
        "  pieces = vocab.encode_as_pieces(line)\n",
        "  ids = vocab.encode_as_ids(line)\n",
        "  inputs.append(torch.tensor(ids))\n",
        "  print(line)\n",
        "  print(pieces)\n",
        "  print(ids)\n",
        "  print() # 단어를 subword 단위로 잘 쪼개는것을 확인할 수 있습니다."
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\r\u001b[K     |▎                               | 10kB 20.9MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 2.4MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |██▌                             | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |██▉                             | 92kB 3.0MB/s eta 0:00:01\r\u001b[K     |███▏                            | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |███▌                            | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |███▉                            | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |████                            | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |████▍                           | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |████▊                           | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 276kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 286kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 296kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 307kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 317kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 327kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 337kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 348kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 358kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 368kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 378kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 389kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 399kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 409kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 419kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 430kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 440kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 450kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 460kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 471kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 481kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 491kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 501kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 512kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 522kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 532kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 542kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 552kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 563kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 573kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 583kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 593kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 604kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 614kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 624kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 634kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 645kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 655kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 665kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 675kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 686kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 696kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 706kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 716kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 727kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 737kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 747kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 757kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 768kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 778kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 788kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 798kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 808kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 819kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 829kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 839kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 849kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 860kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 870kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 880kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 890kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 901kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 911kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 921kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 931kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 942kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 952kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 962kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 972kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 983kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 993kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.0MB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.0MB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.0MB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.0MB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.0MB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.85\n",
            "겨울이 되어서 날씨가 무척 추워요.\n",
            "['▁겨울', '이', '▁되어', '서', '▁날', '씨', '가', '▁무', '척', '▁추', '워', '요', '.']\n",
            "[3099, 3582, 604, 3596, 684, 4009, 3593, 109, 4189, 206, 3951, 3754, 3584]\n",
            "\n",
            "이번 성탄절은 화이트 크리스마스가 될까요?\n",
            "['▁이번', '▁성', '탄', '절', '은', '▁화', '이트', '▁크리스', '마', '스가', '▁될', '까', '요', '?']\n",
            "[3228, 86, 3961, 3917, 3598, 264, 669, 1940, 3658, 777, 1442, 3788, 3754, 4243]\n",
            "\n",
            "겨울에 감기 조심하시고 행복한 연말 되세요.\n",
            "['▁겨울', '에', '▁감', '기', '▁조', '심', '하', '시', '고', '▁행', '복', '한', '▁연', '말', '▁되', '세', '요', '.']\n",
            "[3099, 3585, 208, 3599, 53, 3826, 3590, 3608, 3594, 235, 3865, 3597, 61, 3817, 448, 3676, 3754, 3584]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4lk7-6fHEDR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "426267fa-f051-4741-a58c-4750493f0112"
      },
      "source": [
        "print(len(vocab)) # 8007\n",
        "print(len(ids))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8007\n",
            "18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_MIYQwdNUXJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "b3578e0e-ed44-4a7e-a0cc-1f34c5e94d7b"
      },
      "source": [
        "# 입력 길이가 다르므로 입력 최대 길이에 맟춰 padding(0)을 추가 해 줌\n",
        "inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n",
        "print(inputs)\n",
        "print(inputs.shape) # 3, 18 -> 가장 긴 문장을 기준으로 padding 해준다."
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[3099, 3582,  604, 3596,  684, 4009, 3593,  109, 4189,  206, 3951, 3754,\n",
            "         3584,    0,    0,    0,    0,    0],\n",
            "        [3228,   86, 3961, 3917, 3598,  264,  669, 1940, 3658,  777, 1442, 3788,\n",
            "         3754, 4243,    0,    0,    0,    0],\n",
            "        [3099, 3585,  208, 3599,   53, 3826, 3590, 3608, 3594,  235, 3865, 3597,\n",
            "           61, 3817,  448, 3676, 3754, 3584]])\n",
            "torch.Size([3, 18])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deyDJ6gRI5_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# positional encoding을 해줘야한다.\n",
        "def positional_encoding(seq_len,embedding_dim):\n",
        "    # input shape : [n,seq_len]\n",
        "    #output=torch.zeros((seq_len+1,embedding_dim),device=device)\n",
        "    output=torch.zeros((seq_len,embedding_dim),device=device)\n",
        "    def for_insert(output):\n",
        "        m,n=output.shape\n",
        "        for i in range(m):\n",
        "            for j in range(n):\n",
        "                output[i,j]=i/10000**(2*j/embedding_dim)\n",
        "        return output\n",
        "    pe=for_insert(output)\n",
        "    pe[:,0::2]=torch.sin(pe[:,0::2])\n",
        "    pe[:,1::2]=torch.cos(pe[:,1::2])\n",
        "    # pe[0,:]=0. 없애고 진행해보자\n",
        "    return pe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPNQkNjiL_in",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "e1bad9e3-d651-4948-9208-2e270d2aded3"
      },
      "source": [
        "device='cpu'\n",
        "positional_encoding(64,300)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
              "          0.0000e+00,  1.0000e+00],\n",
              "        [ 8.4147e-01,  5.8943e-01,  7.7356e-01,  ...,  1.0000e+00,\n",
              "          1.1307e-08,  1.0000e+00],\n",
              "        [ 9.0930e-01, -3.0515e-01,  9.8045e-01,  ...,  1.0000e+00,\n",
              "          2.2613e-08,  1.0000e+00],\n",
              "        ...,\n",
              "        [-9.6612e-01,  6.8334e-01, -5.1718e-01,  ...,  1.0000e+00,\n",
              "          6.8970e-07,  1.0000e+00],\n",
              "        [-7.3918e-01, -1.8701e-01, -9.8982e-01,  ...,  1.0000e+00,\n",
              "          7.0101e-07,  1.0000e+00],\n",
              "        [ 1.6736e-01, -9.0380e-01, -7.3737e-01,  ...,  1.0000e+00,\n",
              "          7.1232e-07,  1.0000e+00]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCtihPHIMg7Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "a8d75923-f506-4296-84af-d0cf435e49fb"
      },
      "source": [
        "# 이를 토대로 positional embedding을 구한다.\n",
        "'''\n",
        "위에서 구해진 position encoding 값을 이용해 position embedding을 생성합니다. \n",
        "학습되는 값이 아니므로 freeze옵션을 True로 설정 합니다.\n",
        "입력 inputs과 동일한 크기를 갖는 positions값을 구합니다.\n",
        "input값 중 pad(0)값을 찾습니다.\n",
        "positions값중 pad부분은 0으로 변경 합니다. (mask를 활용)\n",
        "positions값에 해당하는 embedding값을 구합니다\n",
        "'''"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n위에서 구해진 position encoding 값을 이용해 position embedding을 생성합니다. \\n학습되는 값이 아니므로 freeze옵션을 True로 설정 합니다.\\n입력 inputs과 동일한 크기를 갖는 positions값을 구합니다.\\ninput값 중 pad(0)값을 찾습니다.\\npositions값중 pad부분은 0으로 변경 합니다. (mask를 활용)\\npositions값에 해당하는 embedding값을 구합니다\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQmJPNFQMyin",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "81c14d29-e61a-4a4f-8ff7-d6bfbd689c64"
      },
      "source": [
        "############################# input embedding ##########################################\n",
        "n_vocab=len(vocab)\n",
        "embedding=128\n",
        "seq_len=64 # 아싸리 크게 하면 되겠네. 딱 타이트하게 하지 말고...\n",
        "Embedding=nn.Embedding(n_vocab,embedding,padding_idx=0)\n",
        "Embedding.weight.required_grad=False\n",
        "inputs_embedding=Embedding(inputs) \n",
        "############################# positional embedding ##########################################\n",
        "pos_encoding=positional_encoding(seq_len,embedding)\n",
        "positional_embedding=nn.Embedding.from_pretrained(pos_encoding,freeze=True)\n",
        "positions=torch.arange(inputs.size(1)).expand(inputs.size(0),inputs.size(1))+1 # inputs size : n,seq_len\n",
        "print(positions)\n",
        "pos_mask=inputs.eq(0) # mask shape : n,seq_len : 3, 18\n",
        "positions.masked_fill_(pos_mask,0)  # (어차피 0에는 무엇을 곱해도 0이 되니깐)->아니지. 그리고 1을 더했다.. 그전에\n",
        "print(positions)\n",
        "pos_embedding=positional_embedding(positions)\n",
        "print(pos_embedding[1]) # 보게 되면 0에 해당하는 부분은 0,1,0,1로 반복되고 있다."
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18],\n",
            "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18],\n",
            "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18]])\n",
            "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,  0,  0,  0,  0,  0],\n",
            "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,  0,  0,  0,  0],\n",
            "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18]])\n",
            "tensor([[ 8.4147e-01,  6.4791e-01,  6.8156e-01,  ...,  1.0000e+00,\n",
            "          1.3335e-08,  1.0000e+00],\n",
            "        [ 9.0930e-01, -1.6044e-01,  9.9748e-01,  ...,  1.0000e+00,\n",
            "          2.6670e-08,  1.0000e+00],\n",
            "        [ 1.4112e-01, -8.5580e-01,  7.7827e-01,  ...,  1.0000e+00,\n",
            "          4.0006e-08,  1.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
            "          0.0000e+00,  1.0000e+00],\n",
            "        [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
            "          0.0000e+00,  1.0000e+00],\n",
            "        [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
            "          0.0000e+00,  1.0000e+00]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "422ILQbeSWwl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# positional embedding과 token embedding을 더해야지\n",
        "input_sums=inputs_embedding+pos_embedding \n",
        "# anyway, padding이 된 부분에 해당되는 부분이 0 값이 된 것은 아니다.. -> 그렇기 때문에..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LFPW0tIU0Fl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Q=input_sums # shape : 3, 18, 128\n",
        "K=input_sums # inputs shape : 3, 18\n",
        "V=input_sums\n",
        "attention_mask=inputs.eq(0).unsqueeze(1).expand(Q.size(0),Q.size(1),K.size(1)) # shape : 3, 18, 18"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvgZ21i25UKF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_attention_pad_mask(Q,K,padding_idx=0):\n",
        "    # Q : batch_size, query_seq_len\n",
        "    # K : batch_size, key_seq_len\n",
        "    mask=K.eq(padding_idx).unsqueeze(1).expand(K.shape[0],Q.shape[1],K.shape[1])\n",
        "    return mask   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ev-98oGKWfTU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class scaled_dot_product_attention(nn.Module): # 고민 요소\n",
        "    # 나의 초안에서 attention 함수를 차용해 옴\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "               \n",
        "    def forward(self,Q,K,V,attention_mask):\n",
        "        # Q,K - shape [n,seq_len,dk] // V - shape [n,seq_len,dv]\n",
        "        # self attention 이므로 dk=dv(=embedding)\n",
        "        scores=Q.matmul(K.transpose(-2,-1))/torch.sqrt(torch.tensor(Q.shape[-1]).float())\n",
        "        # 여기에서 attention mask는 padding이 된 부분을 0으로 처리한 것\n",
        "        scores.masked_fill_(attention_mask,-1e-9)\n",
        "        weight=F.softmax(scores,dim=-1)\n",
        "        # weight : n, seq_len, seq_len\n",
        "        attention_score=weight.matmul(V)\n",
        "        # attention score : n,seq_len,dv\n",
        "        return attention_score, weight"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aW--ols2aRTC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 나는 multi head attention을 안했었는데, 병렬로 처리하기 위해선 더 좋으니깐\n",
        "# 쩐다...\n",
        "class multi_head_attention(nn.Module): # 고민 요소\n",
        "    # 나의 초안에서 attention 함수를 차용해 옴\n",
        "    def __init__(self,batch_size,n_head,d_head,d_k):\n",
        "        super().__init__()\n",
        "        self.batch_size=batch_size\n",
        "        self.n_head=n_head\n",
        "        self.d_head=d_head\n",
        "        self.d_k=d_k\n",
        "        self.W_Q=nn.Linear(d_k,n_head*d_head)\n",
        "        self.W_K=nn.Linear(d_k,n_head*d_head)\n",
        "        self.W_V=nn.Linear(d_k,n_head*d_head)\n",
        "        self.W_0=nn.Linear(n_head*d_head,n_head*d_head) # n-head*d_head = embedding\n",
        "        self.scdp_attention()\n",
        "    def forward(self,Q,K,V,attention_mask):\n",
        "        # Q,K - shape [n,seq_len,dk] // V - shape [n,seq_len,dv]\n",
        "        # self attention 이므로 dk=dv(=embedding)\n",
        "        # attention_mask shape : [n,seq_len,seq_len]\n",
        "        qs=self.W_Q(Q).reshape(self.batch_size,-1,self.n_head,self.d_head).transpose(1,2)\n",
        "        ks=self.W_K(K).reshape(self.batch_size,-1,self.n_head,self.d_head).transpose(1,2)\n",
        "        vs=self.W_V(V).reshape(self.batch_size,-1,self.n_head,self.d_head).transpose(1,2)\n",
        "        attention_mask=attention_mask.unsqueeze(1).repeat(1,self.n_head,1,1) # attention_mask shape : [n,self.n_head,seq_len,seq_len]\n",
        "        context,weight=self.scdp_attention(qs,ks,vs,attention_mask)\n",
        "        context=context.transpose(1,2).view(self.batch_size,-1,self.n_head*self.d_head)\n",
        "        output=self.W_0(context)\n",
        "        return output, weight"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBJmZemFfWDt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# masked multi_head attention\n",
        "def get_attention_decoder_mask(Q):\n",
        "    # Q : batch_size, query_seq_len\n",
        "    # 어차피 decoder mask는 self attention이므로\n",
        "    mask=torch.ones_like(Q).unsqueeze(1).expand(Q.shape[0],Q.shape[1],Q.shape[1])\n",
        "    # mask : batch_size, seq_len, seq_len\n",
        "    mask=mask.triu(diagonal=1)\n",
        "    return mask   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFaFuUOf3-D8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        },
        "outputId": "e50ea5dc-d6a0-4898-87b4-cef118f53aa4"
      },
      "source": [
        "# pad mask와 decoder mask를 합쳐야 한다.\n",
        "attention_pad_mask=get_attention_pad_mask(inputs)\n",
        "attention_decoder_mask=get_attention_decoder_mask(inputs)\n",
        "print(attention_pad_mask[0].long())\n",
        "print(attention_decoder_mask[0])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]])\n",
            "tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6d6auS76HDM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "f09e83b1-0565-4cff-c503-4799e6b85717"
      },
      "source": [
        "# ? torch.gt()\n",
        "attn_mask=torch.gt((attention_pad_mask+attention_decoder_mask),0) # torch.gt(input,other)=> input>other인 경우 element wise\n",
        "print(attn_mask[0].long())"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYkdHZZI8NyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Config(dict):\n",
        "    __getitem__=dict.__getitem__\n",
        "    __setitem__=dict.__setitem__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irfLmQaE8cOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config=Config({\"n_enc_vocab\": len(vocab),\n",
        "    \"n_dec_vocab\": len(vocab),\n",
        "    \"n_enc_seq\": 256,\n",
        "    \"n_dec_seq\": 256,\n",
        "    \"n_layer\": 6,\n",
        "    \"d_hidden\": 256,\n",
        "    \"padding_idx\": 0,\n",
        "    \"d_feedforward\": 1024,\n",
        "    \"n_head\": 4,\n",
        "    \"d_head\": 64,\n",
        "    \"dropout\": 0.1,\n",
        "    \"layer_norm_epsilon\": 1e-12\n",
        "})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTmvJVMf83RC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "a52515f9-2455-4c05-88e2-f0c6000d2c52"
      },
      "source": [
        "config"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'d_feedforward': 1024,\n",
              " 'd_head': 64,\n",
              " 'd_hidden': 256,\n",
              " 'dropout': 0.1,\n",
              " 'layer_norm_epsilon': 1e-12,\n",
              " 'n_dec_seq': 256,\n",
              " 'n_dec_vocab': 8007,\n",
              " 'n_enc_seq': 256,\n",
              " 'n_enc_vocab': 8007,\n",
              " 'n_head': 4,\n",
              " 'n_layer': 6,\n",
              " 'padding_idx': 0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    }
  ]
}